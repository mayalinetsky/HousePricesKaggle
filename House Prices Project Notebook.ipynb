{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note for Maya\\Yair\\Eran:\n",
    "This notebook is for the final submission, and will contain the entire project flow.\n",
    "In order to update this document please submit a Purll Request from your branch to the master branch, with the other team mates as reviewers.\n",
    "\n",
    "Until then, please develop in a separate notebook, with the name '{your_name}_in_progress.ipynb'.\n",
    "Updates to '...in_progress' notebook should not be pushed to master."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices - Regression Predictions YData 2024\n",
    "Team: TODO\n",
    "\n",
    "Team mates: Eran T, Maya L, Yair BH, Adir Golan.\n",
    "\n",
    "TODO: add table of content with links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T12:55:41.752672500Z",
     "start_time": "2024-01-30T12:55:39.641893100Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import load_house_prices_data\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "train_df = load_house_prices_data('train')\n",
    "train_features = train_df.drop('SalePrice', axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Which 3 features have the highest number of missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T12:55:41.987308500Z",
     "start_time": "2024-01-30T12:55:41.754963200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import calc_num_missing_vals_per_col, np\n",
    "\n",
    "num_of_nans = calc_num_missing_vals_per_col(train_features)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "y_pos = np.arange(len(num_of_nans))\n",
    "ax.barh(y_pos, num_of_nans.values, align='center')\n",
    "ax.set_yticks(y_pos, labels=num_of_nans.index)\n",
    "ax.set_title('Number of Missing Values Per Column')\n",
    "plt.show()\n",
    "\n",
    "max_nans = num_of_nans.nlargest(3).index\n",
    "print(f\"Top 3 features with the most missing values: {max_nans.values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 How does the price behave over the years?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T12:55:42.212050400Z",
     "start_time": "2024-01-30T12:55:41.988129600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import plot_price_dist_per_year\n",
    "\n",
    "plot_price_dist_per_year(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Insight:\n",
    "Over the 4 years shown, the mean price have fluctuated; increasing until 2007, with a down movement overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Plotting feature distribution using histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T13:19:33.696250700Z",
     "start_time": "2024-01-30T13:19:27.595095500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=9, nrows=9, figsize=(20,16))\n",
    "for col_index, col in enumerate(train_df.columns):\n",
    "    ax_col_index = col_index%9\n",
    "    ax_row_index = col_index//9\n",
    "    ax_to_plot = axes[ax_row_index, ax_col_index]\n",
    "    ax_to_plot.set_title(col)\n",
    "    train_df[col].hist(ax=ax_to_plot)\n",
    "\n",
    "plt.suptitle(\"Feature Histograms\")\n",
    "plt.tight_layout()    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph insights:\n",
    "- Many unbalanced categorical features, including \"SaleType\",\"GarageCond\",\"PavedDrive\",\"Street\".\n",
    "- Some numerical feature resemble a normal distribution: \"OverallQual\",\"TotRmsAbvGrd\", \"GarageArea\".\n",
    "- There is a consistent increase in the number of houses built per year.\n",
    "- There is seasonality in the month sold - most sales happen in summer (June, July) and least happen in winter (September, October)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Computing Feature Correlation to Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numeric Featrues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T14:20:14.452847300Z",
     "start_time": "2024-01-30T14:20:14.197561600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr_vector = train_df.select_dtypes(include='number').corr()['SalePrice'].sort_values()\n",
    "corr_vector.drop('SalePrice', axis=0)\n",
    "plt.figure(figsize=(10, 8))\n",
    "corr_vector.plot(kind='barh')\n",
    "\n",
    "plt.title('Correlation to Sale Price of Numerical Features');\n",
    "plt.xlabel(\"Correlation to Sale Price\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph insights:\n",
    "- The number of kitchens above ground has the most negative correlation to the house price.\n",
    "- \"BsmtFinSF2\" has little to no correlation to the price.\n",
    "- \"OverallQual\" has the highest positive correlation, while \"OverallCond\" has somewhat negative correlation, meaning physical condition matters less than subjective measures.\n",
    "- The top 5 features correlated to price mean that people value quality, area of living and garage space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking correlation of target with categorical features (after One-Hot_rncoding) \n",
    "import pandas as pd\n",
    "categorical_columns = train_df.select_dtypes(include=['object']).columns\n",
    "df_encoded = pd.get_dummies(train_df, columns=categorical_columns)\n",
    "correlation = df_encoded.corr()\n",
    "categorical_correlation = correlation['SalePrice'].drop(train_df.select_dtypes(include=[np.number]).columns)\n",
    "sorted_correlation = categorical_correlation.sort_values(ascending=False)\n",
    "\n",
    "sorted_correlation.head(10).plot(kind='barh')\n",
    "plt.title('Top Positive Correlation to Sale Price of Categorical Features (OneHotEncoding)');\n",
    "plt.xlabel(\"Correlation to Sale Price\");\n",
    "plt.show() \n",
    "\n",
    "sorted_correlation.tail(10).plot(kind='barh')\n",
    "plt.title('Top Negative Correlation to Sale Price of Categorical Features (OneHotEncoding)');\n",
    "plt.xlabel(\"Correlation to Sale Price\");\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 More EDA that will help us understand the data and support our modelling decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T12:55:42.288894200Z",
     "start_time": "2024-01-30T12:55:42.240215500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# todo - what will be our modeling decision?\n",
    "# todo - what graphs will support this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LS5DPvxJeqxc",
    "tags": []
   },
   "source": [
    "#### Feature selection (searching features that can be dropped)\n",
    "The idea is that due to the large number of features in the original dataset (80, not including the target), it might be beneficial to reduce the number of features. We do this in differnet ways:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgjgCXDXfNm7"
   },
   "source": [
    "##### Highly correlated numerical features\n",
    "We looked for highly correlated features and decided to drop one of each pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bTKLc9Cu6XJj",
    "outputId": "fa74c557-9584-4a95-874b-034f89e5c674"
   },
   "outputs": [],
   "source": [
    "categorical_columns = train_features.describe(include=[object]).T.index\n",
    "numeric_columns = train_features.describe(include=[np.number]).T.index\n",
    "\n",
    "num_corr = train_features[numeric_columns].corr()\n",
    "correlation_matrix = train_features[numeric_columns].corr()\n",
    "threshold = 0.7\n",
    "highly_correlated_numeric_features = []\n",
    "\n",
    "for i in range(len(numeric_columns)):\n",
    "  for j in range(i+1, len(numeric_columns)):\n",
    "    feature1 = numeric_columns[i]\n",
    "    feature2 = numeric_columns[j]\n",
    "    correlation = correlation_matrix.iloc[i,j]\n",
    "\n",
    "    if correlation > threshold:\n",
    "      highly_correlated_numeric_features.append((feature1, feature2, round(correlation, 3)))\n",
    "\n",
    "print(highly_correlated_numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXBgInBN2GMx"
   },
   "outputs": [],
   "source": [
    "#features to drop due to high correlation with another feature (one from each pair):\n",
    "# we drop features that are not common to all samples (e.g., all buildings must have YearBuilt but not necessarily GarageYrBlt)\n",
    "high_correlated_features_to_drop = ['GarageYrBlt', '1stFlrSF', 'TotRmsAbvGrd', 'GarageCars']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ugo9mbyfs18"
   },
   "source": [
    "##### Correlation of categorical object type features with the target\n",
    "By plotting the categorical distributions of each (categorical) feature with respect to the target, we can choose specific features that seem to hold few meaningful information (mostly features with approximately uniform distribution or highly imbalanced distribtion)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Rwjsa8QB7tVn",
    "outputId": "ba515679-731d-4dc9-9b35-e0b756c9b2a6"
   },
   "source": [
    "# Finding correlation (indirectly) between 'object' features and target:\n",
    "fig, axes = plt.subplots(11,4, figsize=(12,35))\n",
    "for i, column in enumerate(categorical_columns):\n",
    "  target_mean = train_df.groupby(column)['SalePrice'].mean()\n",
    "\n",
    "  target_std = train_df.groupby(column)['SalePrice'].std()\n",
    "\n",
    "  cur_ax = axes[i // 4, i % 4]\n",
    "  target_mean.plot(kind='bar', ax=cur_ax, yerr=target_std, capsize=5)\n",
    "  cur_ax.set_title(column)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9fzNI4F2lbI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Object features that show low correlation to target (by indirect impresion):\n",
    "cat_cols_uncor_w_target = ['LotShape', 'LandContour', 'LotConfig',\n",
    "                           'LandSlope', 'Condition2', 'RoofMatl', 'BsmtExposure',\n",
    "                           'BsmtFinType1', 'BsmtFinType2', 'Electrical',\n",
    "                           'Functional', 'Fence', 'MiscFeature'\n",
    "                           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a25m3CbGiSFw"
   },
   "source": [
    "##### Numerical features with imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVlwx9WWAG0N"
   },
   "outputs": [],
   "source": [
    "#numerical features to drop due to high imbalance of the data:\n",
    "drop_imbalanced = ['Heating', 'Alley', 'Street', 'Utilities']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TEQk8wGZbCKF"
   },
   "outputs": [],
   "source": [
    "# features with NaN values that relflect 'None' and should not be discraded (should be counted):\n",
    "convert_nan_to_str = ['BsmtQual', 'BsmtCond', 'FireplaceQu', 'GarageType',\n",
    "                      'GarageFinish', 'GarageQual', 'GarageCond'\n",
    "                      ]\n",
    "\n",
    "# features with same problem, but were already dropped due to other reasons:\n",
    "#['BsmtExposure', 'BsmtFinType1', 'PoolQC', 'MiscFeature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZeD1jDtq7ICn"
   },
   "outputs": [],
   "source": [
    "# filtering the data frame according to selected features to drop:\n",
    "filtered_df = train_df.drop(high_correlated_features_to_drop, axis=1)\n",
    "filtered_df = filtered_df.drop(cat_cols_uncor_w_target, axis=1)\n",
    "filtered_df.drop(drop_imbalanced, axis=1, inplace=True)\n",
    "\n",
    "for feature in convert_nan_to_str:\n",
    "  filtered_df[feature].fillna(value='No', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxndRuEljBkl"
   },
   "source": [
    "##### Feature engineering on pool information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7t42tmMB966J"
   },
   "outputs": [],
   "source": [
    "# only 7 samples with pool, but might be important, so:\n",
    "# we create new *binary* feature 'HavePool' and drop 'PoolQC' 'PoolArea'\n",
    "\n",
    "filtered_df['HavePool'] = filtered_df['PoolArea']\n",
    "filtered_df.loc[filtered_df['HavePool'] != 0, 'HavePool'] = 1\n",
    "filtered_df.loc[filtered_df['HavePool'] == 0, 'HavePool'] = 0\n",
    "filtered_df.drop(['PoolArea', 'PoolQC'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5szPvHBpjZzm"
   },
   "source": [
    "##### Treating missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWyfH2VLvPwf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# replacing missing values in 'LotFrontage' with mean values:\n",
    "mean_value_LotFrontage = filtered_df['LotFrontage'].mean()\n",
    "filtered_df['LotFrontage'].fillna(value=mean_value_LotFrontage, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Understanding The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Looking for seasonality in number of sales \n",
    "sales_grouped = filtered_df.groupby(['YrSold', 'MoSold']).size()\n",
    "sales_grouped_reset = sales_grouped.reset_index(name='Count')\n",
    "sales_grouped_reset['Year-Month'] = sales_grouped_reset['YrSold'].astype(str) + '-' + sales_grouped_reset['MoSold'].astype(str)\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(sales_grouped_reset['Year-Month'], sales_grouped_reset['Count'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Sales Seasonality')\n",
    "plt.xlabel('Year-Month')\n",
    "plt.ylabel('Number of Sales')\n",
    "plt.grid(True)\n",
    "\n",
    "#looking for seasonality in sale prices \n",
    "price_grouped = filtered_df.groupby(['YrSold', 'MoSold'])['SalePrice'].mean()\n",
    "price_grouped_reset = price_grouped.reset_index(name='AvgPrice')\n",
    "price_grouped_reset['Year-Month'] = price_grouped_reset['YrSold'].astype(str) + '-' + price_grouped_reset['MoSold'].astype(str)\n",
    "plt.figure(figsize=(12, 3))\n",
    "plt.plot(price_grouped_reset['Year-Month'], price_grouped_reset['AvgPrice'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Price Seasonality')\n",
    "plt.xlabel('Year-Month')\n",
    "plt.ylabel('Average Sale Price')\n",
    "plt.xlim('2006-1', '2010-6')\n",
    "plt.ylim(150000)\n",
    "plt.grid(True)\n",
    "\n",
    "# Looking for correlation in both seasonality patterns \n",
    "fig, ax1 = plt.subplots(figsize=(12, 3))\n",
    "\n",
    "ax1.plot(sales_grouped_reset['Year-Month'], sales_grouped_reset['Count'], label='Sales Count', color='blue')\n",
    "ax1.set_xlabel('Year-Month')\n",
    "ax1.set_ylabel('Number of Sales', color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(price_grouped_reset['Year-Month'], price_grouped_reset['AvgPrice'], label='Average Price', color='red')\n",
    "ax2.set_ylabel('Average Price', color='red')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "lines, labels = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax2.legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "ax2.set_ylim(150000)\n",
    "plt.title('Sales and Average Price Seasonality')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The plots above demonstrate pretty clear seasonality in two features: \n",
    "* Sales seasonality: we can see a peak in the number of sales on a yearly basis around May-June, followed by a decrease in sales from June to January, with mainly January as the weakest month. \n",
    "* Price seasonality: We can see some seasonality in sales prices, albeit less consistent than in the number of sales case. \n",
    "\n",
    "Instrestingly, by looking at the combined plot we can see some periods where the number of sales drops down drastically while average price hits a peak. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T12:55:42.304195200Z",
     "start_time": "2024-01-30T12:55:42.254320800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# todo train the simplest baseline model possible\n",
    "# todo share baseline model results\n",
    "# todo submit your baseline results to kaggle website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "simple_linear_model = LinearRegression()\n",
    "train, validation = train_test_split(train_df, test_size=0.2, random_state=0)\n",
    "train = train.select_dtypes(include='number')\n",
    "validation = validation.select_dtypes(include='number')\n",
    "\n",
    "simple_imputer = SimpleImputer(missing_values=pd.NA, strategy='mean')\n",
    "simple_imputer.set_output(transform='pandas')\n",
    "train = simple_imputer.fit_transform(train)\n",
    "validation = simple_imputer.fit_transform(validation)\n",
    "\n",
    "simple_linear_model.fit(train.drop('SalePrice', axis = 1), train['SalePrice'])\n",
    "y_pred = simple_linear_model.predict(validation.drop('SalePrice', axis = 1))\n",
    "y_true = np.array(validation['SalePrice'])\n",
    "# print(y_pred < 0)\n",
    "# print(y_true < 0)\n",
    "\n",
    "rmse = mean_squared_error(y_true, y_pred)\n",
    "print(np.sqrt(rmse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
